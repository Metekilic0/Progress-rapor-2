{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4661c4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "BASE_PATH = \"C:/Users/mkasl/Desktop/donem5/yap470/Ara_rapor_ 1/final_dataset1\" \n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
    "VALID_PATH = os.path.join(BASE_PATH, 'valid')\n",
    "TEST_PATH = os.path.join(BASE_PATH, 'test')\n",
    "\n",
    "IMAGE_SIZE = (128, 128) \n",
    "BATCH_SIZE = 32\n",
    "tf.random.set_seed(42) #aynı sonucu vermesi için bunu gerçi dosyalara test train valid olarak ayırıyoruz\n",
    "#tekrarlanabilirliği sağlıyor\n",
    "EPOCHS = 30 \n",
    "#%70 %15 %15 dağılım\n",
    "#Veri setlerini yükleyip hazırlama categorical olması için one hot encoding yapıyorum\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_PATH, labels='inferred', label_mode='categorical',\n",
    "    image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, shuffle=True, seed=42)\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    VALID_PATH, labels='inferred', label_mode='categorical',\n",
    "    image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TEST_PATH, labels='inferred', label_mode='categorical',\n",
    "    image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "#data augmentation yeni veriler üretip yapay olarak overfittingi önlemeye çalışıyorum\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "#Burası cache gibi önbellekleme vs. kullanıp performansı arttırmaya çalıştığım kısım ama buna rağmen daha farklı şeylere\n",
    "#ihtiyacım var prefecthi sonraki batchi hazırlar cache önbelleğe alır\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential(name=\"V20 Model\")\n",
    "model.add(tf.keras.layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)))\n",
    "#1. Blok\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "#2. Blok\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "#3. Blok\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "#4. Blok\n",
    "model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name=\"conv4\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(name=\"bn4\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), name=\"pool4\"))\n",
    "#5. Blok\n",
    "model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name=\"conv5\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(name=\"bn5\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), name=\"pool5\"))\n",
    "#GlobalAveragePooling2D ile sınıflandırma eskiden burasında flatten ve dense katmanı vardı\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(len(class_names), activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "#Bu modelimde değiştirdiğim kısım\n",
    "#Veri setindeki dosya sayılarını buluyorum ardından sınıfları sayıya göre ağırlıklandırıyorum\n",
    "num_birds = len(os.listdir(os.path.join(TRAIN_PATH, 'bird'))) #sayma\n",
    "num_drones = len(os.listdir(os.path.join(TRAIN_PATH, 'drone'))) #sayma\n",
    "total_train_samples = num_birds + num_drones\n",
    "#Ağırlıkları hesaplama\n",
    "weight_for_bird = (1 / num_birds) * (total_train_samples / 2.0)\n",
    "weight_for_drone = (1 / num_drones) * (total_train_samples / 2.0)\n",
    "class_weights = {class_names.index('bird'): weight_for_bird, class_names.index('drone'): weight_for_drone}\n",
    "#default olan adam optimizer kullanıyorum\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#EarlyStopping, Checkpoint ve Dinamik öğrenme callbackleri v10 modelimden dinamik öğrenme\n",
    "callbacks = [\n",
    "    # patience=5 olarak güncelledim öğrenme oranına ve sınıflandırmaya adapte olması için\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='best_model.keras', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, min_lr=0.00001)\n",
    "]\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=EPOCHS,\n",
    "  callbacks=callbacks,\n",
    "  class_weight=class_weights\n",
    ")\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "actual_epochs = len(acc) \n",
    "epochs_range = range(actual_epochs) #X eksenini dinamik olarak ayarlıyorum.\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset) #accuracy hesaplama \n",
    "print(f\"\\nTest Seti Doğruluğu: {test_accuracy:.4f}\")\n",
    "print(f\"Test Seti Kaybı: {test_loss:.4f}\")\n",
    "# Karmaşıklık matrisi\n",
    "y_true_labels = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "y_true = np.argmax(y_true_labels, axis=1)\n",
    "#Test setindeki tüm resimler için tahminleri al\n",
    "y_pred_probs = model.predict(test_dataset)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Karmaşıklık Matrisi')\n",
    "plt.ylabel('Gerçek Etiket')\n",
    "plt.xlabel('Tahmin Edilen Etiket')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
