{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2df28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "DATASET_PATH = \"C:/Users/mkasl/Desktop/donem5/yap470/Ara_rapor_ 1/dataset1_clean\"\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "#%70 %15 %15 dağılım\n",
    "VALIDATION_SPLIT_RATIO = 0.15\n",
    "TEST_SPLIT_RATIO = 0.15\n",
    "\n",
    "tf.random.set_seed(42) #aynı sonucu vermesi için bunu gerçi dosyalara test train valid olarak ayırıyoruz\n",
    "#tekrarlanabilirliği sağlıyor\n",
    "full_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_PATH,\n",
    "    labels='inferred',           \n",
    "    label_mode='categorical', #Etiketler one-hot encoding formatına çevrilir çünkü bird drone vs. saçma \n",
    "    image_size=IMAGE_SIZE,  #ortak boyut tutmak için\n",
    "    batch_size=BATCH_SIZE,       \n",
    "    shuffle=True,                \n",
    "    seed=42\n",
    ")\n",
    "class_names = full_dataset.class_names #Görselleştirmede kullanıyorum \n",
    "#Toplam batch sayısını bulma\n",
    "dataset_size = tf.data.experimental.cardinality(full_dataset).numpy()\n",
    "#Oranlara göre batch sayısı hesaplama\n",
    "val_size = int(dataset_size * VALIDATION_SPLIT_RATIO)\n",
    "test_size = int(dataset_size * TEST_SPLIT_RATIO)\n",
    "train_size = dataset_size - val_size - test_size\n",
    "#Önce test setini ayır sonra valid sonra train \n",
    "test_dataset = full_dataset.take(test_size)\n",
    "validation_dataset = full_dataset.skip(test_size).take(val_size)\n",
    "train_dataset = full_dataset.skip(test_size + val_size)\n",
    "#data augmentation yeni veriler üretip yapay olarak overfittingi önlemeye çalışıyorum\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\", input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "#Burası cache gibi önbellekleme vs. kullanıp performansı arttırmaya çalıştığım kısım ama buna rağmen daha farklı şeylere\n",
    "#ihtiyacım var prefecthi sonraki batchi hazırlar cache önbelleğe alır\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential(name=\"V12 model\")\n",
    "model.add(tf.keras.layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)))\n",
    "#1. blok\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name=\"conv1\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(name=\"bn1\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), name=\"pool1\"))\n",
    "#2. blok\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name=\"conv2\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(name=\"bn2\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), name=\"pool2\"))\n",
    "#3. blok\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name=\"conv3\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(name=\"bn3\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2), name=\"pool3\"))\n",
    "#GlobalAveragePooling2D ile sınıflandırma eskiden burasında flatten ve dense katmanı vardı\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D(name=\"global_pool\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5, name=\"dropout\"))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax', name=\"output\"))\n",
    "model.summary()\n",
    "\n",
    "#default olan adam optimizer kullanıyorum\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "#Bu modelimde değiştirdiğim kısım\n",
    "#Veri setindeki dosya sayılarını buluyorum ardından sınıfları sayıya göre ağırlıklandırıyorum\n",
    "path_bird = os.path.join(DATASET_PATH, 'bird') #sayma\n",
    "path_drone = os.path.join(DATASET_PATH, 'drone') #sayma\n",
    "num_birds = len(os.listdir(path_bird))\n",
    "num_drones = len(os.listdir(path_drone))\n",
    "total_samples = num_birds + num_drones\n",
    "#Ağırlıkları hesaplama\n",
    "weight_for_bird = (1 / num_birds) * (total_samples / 2.0)\n",
    "weight_for_drone = (1 / num_drones) * (total_samples / 2.0)\n",
    "\n",
    "#Ağrılıklar\n",
    "class_weights = {\n",
    "    class_names.index('bird'): weight_for_bird,\n",
    "    class_names.index('drone'): weight_for_drone\n",
    "}\n",
    "\n",
    "# patience=5 olarak güncelledim öğrenme oranına ve sınıflandırmaya adapte olması için\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "#En iyi modeli kaydeder earlystopping için gerekli.\n",
    "model_checkpoint = ModelCheckpoint(filepath='best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "#çok iyi başarım gösteren v10 modelimden dinamik öğrenme\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, min_lr=0.00001) #dinamik öğrenme\n",
    "\n",
    "EPOCHS = 30 #EarlyStopping'in \n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=EPOCHS,\n",
    "  callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "  class_weight=class_weights #Bu modelde yeni eklediğim parametre\n",
    ")\n",
    "#history'den eğitim ve validasyon metriklerini alıyorum.\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "#Gerçekte kaç epoch çalıştığını history listesinin uzunluğundan anlıyorum.\n",
    "actual_epochs = len(acc) \n",
    "epochs_range = range(actual_epochs) #X eksenini dinamik olarak ayarlıyorum.\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset) #accuracy hesaplama \n",
    "print(f\"\\nTest Seti Doğruluğu: {test_accuracy:.4f}\")\n",
    "print(f\"Test Seti Kaybı: {test_loss:.4f}\")\n",
    "#Test setindeki tüm resimler için tahminleri al\n",
    "y_pred_probs = model.predict(test_dataset) #karmaşıklık matrisi\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "#gerçek etiketleri al\n",
    "y_true = []\n",
    "for images, labels in test_dataset:\n",
    "  y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true = np.array(y_true)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Karmaşıklık Matrisi')\n",
    "plt.ylabel('Gerçek Etiket')\n",
    "plt.xlabel('Tahmin Edilen Etiket')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
